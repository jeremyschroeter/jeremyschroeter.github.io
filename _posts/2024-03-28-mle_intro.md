---
layout: post
title: An Introduction to Maximum Likelihood Estimation
subtitle: Discussing maximum likelihood estimation as an excuse to test out LaTeX rendering.
---

{% katexmm %}
In statistics, maximum likelihood estimation (MLE) is a fundamental technique used to estimate the parameters of a presumed probability distribution based on observed data. By relying on a few important assumptions about the data generating process, MLE allows us to derive parameter estimates that maximize the probability of observing the given data. Here, I go over the basics of MLE as the first part in a series of posts explaining the connection between MLE and least squares regression.
To begin, let's start with a concrete example and then we'll try to generalize what we learn from it.
## Flipping Biased Coins
Imagine you are given a biased coin and are tasked with approximating the probability with which it lands heads or tails. How do you go about doing this? Your intuition probably tells you to observe a bunch of coin flips and simply count the frequency of heads or tails. Our gut tells us that for a sufficiently high number of flips these frequencies will begin to approach the true bias of the coin. But is there a way to reason our way there without relying purely on intuition? Let's try.
<br><br>
Firstly, since we know there are only two possible outcomes we can say that if $P(H) = \theta$ then it must be the case that $P(T) = 1 - \theta$. Additionally, we know that we have to observe a large number of flips before we can say much of anything about the probabilities, and we also know that these flips are independent of one another. Let's call the number of flips we observe $n$. Finally, if you've been exposed to basic probability theory you might remember that the probability of two independent events co-occuring is the product of their individual probabilities [A1](#appendix_one).
$$ P(x)\perp P(y) \rightarrow P(x, y) = P(x)P(y)$$
With this information we can write the probability of observing a particular sequence of $n$ coin flips which we denote as $\mathcal{D}$ given a particular probability of flipping heads $\theta$:
$$ P(\mathcal{D} \mid \theta) = \theta^k(1 - \theta)^{n - k} $$
where $k$ is the number of times we flip heads and $n - k$ is the number of times we flip tails.
<br><br>
Now, ask yourself the following question: given some observed set of flips $\mathcal{D}$, what value for $\theta$ maximizes $P(\mathcal{D})$. In other words, what value of $\theta$ makes our observed dataset most probable? That number *is* the MLE estimate.
$$\hat{\theta}_{\text{MLE}}=\argmax_\theta{P(\mathcal{D}\mid \theta)}$$
Said another way, given a set of observed flips, what value for $P(H)$ could we choose that would maximize the chances of observing those outcomes?
<br><br>
At first encounter the MLE estimate can often feel slippery or circular. We are using our data to approximate a model that best explains...our data. However, MLE is really the operationalization of a deep intuition: *given an outcome, what conditions would have most likely led to that outcome?* The mathematical formulation may not be as intuitive, but the underlying idea is beautifully straightforward.
<br>
<blockquote style="font-style: italic">
"At a superficial level, the idea of maximum likelihood must be prehistoric: early hunters and gatherers may not have used the words 'method of maximum likelihood' to describe their choice of where and how to hunt and gather, but it is hard to believe they would have been surprised if their method had been described in those terms."<sub><a href="https://arxiv.org/pdf/0804.2996.pdf">1</a></sub>
</blockquote>
If we have some data, and we are willing to assume that it comes from a particular class of distribution (a normal distribution, for example), then the MLE simply asks what parameters (mean and variance, for example) to give that distribution to make it **most consistent** with our data.
<br><br>
Okay, lets see the MLE in action. We are tasked with solving the following optimization problem
$$ \argmax_{\theta}P(\mathcal{D}\mid\theta) = \argmax_{\theta}\left[\theta^k(1 - \theta)^{n - k}\right]$$
The solution to said problem will be our MLE estimate $\hat{\theta}_{\text{MLE}}$ which is the approximate probability of our biased coin flipping heads. To start we can notice that whatever argument maximizes our function will also maximize its logarithm. This is because the logarithm is a [monotonically increasing function](https://en.wikipedia.org/wiki/Monotonic_function?oldformat=true). Reframing the problem this way will allow us to get rid of those exponents and turn multiplication into addition.
$$\begin{aligned}
\argmax_{\theta}\left[\theta^k(1 - \theta)^{n - k}\right] &= \argmax_{\theta}\left[\log(\theta^k(1 - \theta)^{n - k})\right] \\
&= \argmax_{\theta}\left[\log(\theta^k) + \log((1-\theta)^{n-k})\right]\\
&= \argmax_{\theta}\left[k\log\theta + (n-k)\log(1-\theta)\right]
\end{aligned}$$
So far all we have done is some algebraic manipulation of $P(\mathcal{D} \mid \theta)$. Now to solve the optimization problem, we can take the derivative of our function and solve for its maximum by setting it equal to zero.
$$
\begin{aligned}
    \frac{d}{d\theta}\log P(\mathcal{D}\mid\theta) &= \frac{d}{d\theta}\left[k\log\theta + (n-k)\log(1-\theta)\right]\\
    &=\frac{k}{\theta} - \frac{n-k}{1-\theta}
\end{aligned}
$$
Now we set our equation equal to $0$ and solve for $\hat{\theta}_{\text{MLE}}$
$$
\begin{aligned}
0 &= \frac{k}{\theta} - \frac{n-k}{1-\theta}\\
&=\frac{k(1-\theta)\theta}{\theta} - \frac{\theta(n-k)(1-\theta)\theta}{1-\theta}\\
&= k(1 - \theta) - \theta(n - k)\\
&= k - k\theta - n\theta + k\theta \\
&= k - n\theta\\
\hat{\theta}_{\text{MLE}} &= \frac{k}{n}
\end{aligned}
$$
And so at the end of all of that we find that the maximum likelihood estimate for the probability of flipping heads given a dataset is simply the frequency of head occurences. Our intuitions have been confirmed! That was a lot of math for such a simple answer, but granted this was a pretty simple problem. Can the MLE generalize to a larger problem space where it's harder for our intuition to guide us? The answer is ***yes***, but before we get there, lets simulate some coin flips and see how our estimate does as $n$ increases.
```py
import matplotlib.pyplot as plt
import numpy as np
# Parameters
p_true = 0.65   # True probability of getting a head
n_flips = 1000  # Number of coin flips
trials = 50   # Do 50 trials with 1000 flips
trial_outcomes = []
for trial in range(trials):
    # Simulate coin flips
    outcomes = np.random.binomial(n=1, p=p_true, size=n_flips)  # 1 is heads, 0 is tails
    # Calculate cumulative mean of outcomes
    cumulative_mean = np.cumsum(outcomes) / np.arange(1, n_flips + 1)
    trial_outcomes.append(cumulative_mean)
# Plotting
plt.style.use('ggplot')
plt.rcParams['figure.dpi'] = 250
plt.figure(figsize=(10, 6))
for outcome in trial_outcomes:
    plt.plot(outcome, lw=0.6)
plt.axhline(y=p_true, color='black', linestyle='--', label='True Probability of Heads')
plt.title('Cumulative Mean Estimate of Heads Probability')
plt.xlabel('Number of Coin Flips')
plt.ylabel('Cumulative Mean Probability of Heads')
plt.legend()
plt.show()
```
<div style="text-align: center; transform: translateX(-6.5%);">
<img src="/assets/images/cumulative_mean.png" width="750">
</div>
## Generalizing the MLE 
We begin by observing a set of data points $$X_1, X_2, \dots, X_n$$
We assume these data points are sampled **independently** from a **known class** of probability distributions $f(x, \theta^*)$.  Ultimately, what we want from MLE is an accurate approximation of $\theta^*$ which we will refer to as $\hat{\theta}$. Crucial to getting there will be the two highlighted assumptions: **(1)** that our data points are independent of one another, and **(2)** the form of the presumed distribution is a good model of those data points.
### Appendix
<a name="appendix_one"></a>A1.<br>
If this seems unintuitive to you, imagine you have a pair of unweighted $6$-sided die and want to know the probability of rolling a $6$ with both of them. If you were to roll a $1$ first, you haven't effected the outcome of the second roll and so there are still $6$ possible outcomes. This is actually true no matter what number we roll first and so for each outcome of the first roll we have $6$ possible outcomes for the second. Thus our total number of possibilities is $6 \cdot 6 = 36$ and so the probability of any one outcome is $\frac{1}{36}$. In fact, if we had $3$ die then we would have $6^3$ possibilities each with probability $\frac{1}{6^3}$. Or if we added a coin flip we would have $2 \cdot 6^2$ possibilities each with probability $\frac{1}{72}$ because for each of the $36$ outcomes from the die rolling we could flip either heads or tails. All of this is a function of the events being independent from one another, and would not work if the outcomes of one process altered the outcomes of another.
{% endkatexmm %}